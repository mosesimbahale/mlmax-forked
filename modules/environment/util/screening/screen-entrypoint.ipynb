{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook screens that it can perform submit Python script as *train* and *processing*. It is designed to run in one go without a kernel restart, hence submits only short training and batch-transform jobs each of which runs for 3+ minutes.\n",
    "\n",
    "Steps:\n",
    "- **Action**: click *Kernel* -> *Restart Kernel and Run All Cells...* \n",
    "- **Expected outcome**: no exception seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before you run the next cell, please open `smconfig.py` and review the mandatory SageMaker `kwargs` then disable the `NotImplementedException` in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import sagemaker as sm\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "import smconfig\n",
    "\n",
    "# Configuration of this screening test.\n",
    "sess = sm.Session()\n",
    "sm_kwargs = smconfig.SmKwargs(sm.get_execution_role())\n",
    "s3_input_path = f'{smconfig.s3_bucket}/screening/entrypoint-input'\n",
    "s3_sagemaker_path = f'{smconfig.s3_bucket}/screening/sagemaker'\n",
    "\n",
    "# Enforce blocking API to validate permissions to Describe{Training,Transform}Job.\n",
    "block_notebook_while_training = True\n",
    "\n",
    "# Propagate to env vars of the whole notebook, for usage by ! or %%.\n",
    "%set_env BUCKET=$smconfig.s3_bucket\n",
    "%set_env S3_INPUT_PATH=$s3_input_path\n",
    "%set_env S3_SAGEMAKER_PATH=$s3_sagemaker_path\n",
    "\n",
    "# Create dummy input file\n",
    "!echo \"Dummy input file\" | aws s3 cp - $S3_INPUT_PATH/input.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = MXNet(\n",
    "    entry_point='screen.py',\n",
    "    source_dir='./sourcedir_screen',\n",
    "    framework_version='1.6.0',\n",
    "    py_version='py3',\n",
    "\n",
    "    # sourcedir.tar.gz and output use pre-defined bucket.\n",
    "    code_location=s3_sagemaker_path,\n",
    "    output_path=s3_sagemaker_path,\n",
    "\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    sagemaker_session=sess,\n",
    "    **sm_kwargs.train,\n",
    ")\n",
    "\n",
    "# Submit a training job.\n",
    "estimator.fit({'train': s3_input_path}, wait=block_notebook_while_training)\n",
    "\n",
    "# Track the jobname for subsequent CloudWatch CLI operations.\n",
    "train_job_name = estimator.latest_training_job.name\n",
    "%set_env TRAIN_JOB_NAME=$estimator.latest_training_job.name\n",
    "\n",
    "# Probe output\n",
    "!aws s3 cp $S3_SAGEMAKER_PATH/$TRAIN_JOB_NAME/output/output.tar.gz - | tar --to-stdout -xzf - screenings.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    sagemaker_session=sess,\n",
    "    **sm_kwargs.processing,\n",
    ")\n",
    "\n",
    "# Manually upload the code to a specific S3 bucket, otherwise SageMaker SDK\n",
    "# always uploads to default_bucket() `s3://sagemaker-{}-{}/`.\n",
    "!aws s3 cp sourcedir_screen/screen.py $S3_SAGEMAKER_PATH/processing-code/screen.py\n",
    "\n",
    "# Generate job name and track it. We need to do this to set the S3 output path\n",
    "# to s3://mybucket/...../jobname/output/....\n",
    "#\n",
    "# See: https://github.com/aws/sagemaker-python-sdk/blob/570c67806f4f85f954d836d01c6bb06a24b939ee/src/sagemaker/processing.py#L315\n",
    "processing_job_name = processor._generate_current_job_name()\n",
    "%set_env PROCESSING_JOB_NAME=$processing_job_name\n",
    "\n",
    "# Submit a processing job.\n",
    "processor.run(\n",
    "    job_name=processing_job_name,\n",
    "    code=f'{s3_sagemaker_path}/processing-code/screen.py',\n",
    "    inputs=[ProcessingInput(source=s3_input_path, destination='/opt/ml/processing/input')],\n",
    "    outputs=[ProcessingOutput(source='/opt/ml/processing/output', destination=f'{s3_sagemaker_path}/{processing_job_name}/output')],\n",
    "    arguments=[\"--module\", \"sklearn\"],\n",
    "    wait=block_notebook_while_training,\n",
    ")\n",
    "\n",
    "# Probe output\n",
    "!aws s3 cp $S3_SAGEMAKER_PATH/$PROCESSING_JOB_NAME/output/screenings.jsonl -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: CloudWatch log events\n",
    "\n",
    "You can retrieve the training logs using `awscli` after this notebook is unblocked. This will be a good test to verify that this notebook's role has sufficient permissions to read CloudWatch logs.\n",
    "\n",
    "Assuming the job name is stored in an environment variable `TRAIN_JOB_NAME`, run these CLI commands:\n",
    "\n",
    "```bash\n",
    "# Find out the log-stream name; should look like TRAIN_JOB_NAME/xxx.\n",
    "aws logs describe-log-streams \\\n",
    "    --log-group-name /aws/sagemaker/TrainingJobs \\\n",
    "    --log-stream-name-prefix $TRAIN_JOB_NAME \\\n",
    "    | jq -r '.logStreams[].logStreamName'\n",
    "\n",
    "\n",
    "# Get the log events\n",
    "aws logs get-log-events \\\n",
    "    --log-group-name /aws/sagemaker/TrainingJobs \\\n",
    "    --log-stream-name <LOG_STREAM_NAME>\n",
    "```\n",
    "\n",
    "For processing job, the log group name is `/aws/sagemaker/ProcessingJobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
