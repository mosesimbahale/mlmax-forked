{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook screens that it can perform submit Python script as *train* and *processing* with an **input manifest file**. It is designed to run in one go without a kernel restart, hence submits only short training and batch-transform jobs each of which runs for 3+ minutes.\n",
    "\n",
    "Steps:\n",
    "- **Action**: click *Kernel* -> *Restart Kernel and Run All Cells...* \n",
    "- **Expected outcome**: no exception seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before you run the next cell, please open `smconfig.py` and review the mandatory SageMaker `kwargs` then disable the `NotImplementedException` in the last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import sagemaker as sm\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "import smconfig\n",
    "\n",
    "# Configuration of this screening test.\n",
    "sess = sm.Session()\n",
    "# sm_kwargs = smconfig.SmKwargs(sm.get_execution_role())\n",
    "sm_kwargs = smconfig.SmKwargs(\n",
    "    \"arn:aws:iam::484597657167:role/service-role/AmazonSageMaker-ExecutionRole-20180516T132752\"\n",
    ")\n",
    "s3_input_path = f\"{smconfig.s3_bucket}/screening/entrypoint-input\"\n",
    "s3_input_manifest = f\"{s3_input_path}/input-manifest.txt\"\n",
    "s3_sagemaker_path = f\"{smconfig.s3_bucket}/screening/sagemaker\"\n",
    "\n",
    "# Enforce blocking API to validate permissions to Describe{Training,Transform}Job.\n",
    "block_notebook_while_training = True\n",
    "\n",
    "# Propagate to env vars of the whole notebook, for usage by ! or %%.\n",
    "%set_env BUCKET=$smconfig.s3_bucket\n",
    "%set_env S3_INPUT_PATH=$s3_input_path\n",
    "%set_env S3_INPUT_MANIFEST=$s3_input_manifest\n",
    "%set_env S3_SAGEMAKER_PATH=$s3_sagemaker_path\n",
    "\n",
    "# Create dummy input files\n",
    "!echo \"Dummy input file 01\" | aws s3 cp - $S3_INPUT_PATH/input-01.txt\n",
    "!echo \"Dummy input file 02\" | aws s3 cp - $S3_INPUT_PATH/input-02.txt\n",
    "!echo \"Dummy input file 03\" | aws s3 cp - $S3_INPUT_PATH/input-03.txt\n",
    "\n",
    "# Create manifest file\n",
    "!echo \"[{\\\"prefix\\\": \\\"$S3_INPUT_PATH/\\\"}, \\\"input-01.txt\\\", \\\"input-03.txt\\\"]\" | aws s3 cp - $S3_INPUT_MANIFEST\n",
    "!aws s3 cp $S3_INPUT_MANIFEST -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    entry_point=\"screen.py\",\n",
    "    source_dir=\"./sourcedir_screen\",\n",
    "    framework_version=\"1.8.0\",\n",
    "    py_version=\"py3\",\n",
    "    hyperparameters={\"module\": \"torch\"},\n",
    "    # sourcedir.tar.gz and output use pre-defined bucket.\n",
    "    code_location=s3_sagemaker_path,\n",
    "    output_path=s3_sagemaker_path,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sess,\n",
    "    **sm_kwargs.train,\n",
    ")\n",
    "\n",
    "# Submit a training job.\n",
    "estimator.fit(\n",
    "    {\"train\": TrainingInput(s3_input_manifest, s3_data_type=\"ManifestFile\")},\n",
    "    wait=block_notebook_while_training,\n",
    ")\n",
    "\n",
    "# Track the jobname for subsequent CloudWatch CLI operations.\n",
    "train_job_name = estimator.latest_training_job.name\n",
    "%set_env TRAIN_JOB_NAME=$estimator.latest_training_job.name\n",
    "\n",
    "# Probe output\n",
    "!aws s3 cp $S3_SAGEMAKER_PATH/$TRAIN_JOB_NAME/output/output.tar.gz - | tar --to-stdout -xzf - screenings.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=sess,\n",
    "    **sm_kwargs.processing,\n",
    ")\n",
    "\n",
    "# Manually upload the code to a specific S3 bucket, otherwise SageMaker SDK\n",
    "# always uploads to default_bucket() `s3://sagemaker-{}-{}/`.\n",
    "!aws s3 cp sourcedir_screen/screen.py $S3_SAGEMAKER_PATH/processing-code/screen.py\n",
    "\n",
    "# Generate job name and track it. We need to do this to set the S3 output path\n",
    "# to s3://mybucket/...../jobname/output/....\n",
    "#\n",
    "# See: https://github.com/aws/sagemaker-python-sdk/blob/570c67806f4f85f954d836d01c6bb06a24b939ee/src/sagemaker/processing.py#L315\n",
    "processing_job_name = processor._generate_current_job_name()\n",
    "%set_env PROCESSING_JOB_NAME=$processing_job_name\n",
    "\n",
    "# Submit a processing job.\n",
    "processor.run(\n",
    "    job_name=processing_job_name,\n",
    "    code=f\"{s3_sagemaker_path}/processing-code/screen.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=s3_input_manifest,\n",
    "            s3_data_type=\"ManifestFile\",\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=f\"{s3_sagemaker_path}/{processing_job_name}/output\",\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\"--module\", \"sklearn\"],\n",
    "    wait=block_notebook_while_training,\n",
    ")\n",
    "\n",
    "# Probe output\n",
    "!aws s3 cp $S3_SAGEMAKER_PATH/$PROCESSING_JOB_NAME/output/screenings.jsonl -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: CloudWatch log events\n",
    "\n",
    "You can retrieve the training logs using `awscli` after this notebook is unblocked. This will be a good test to verify that this notebook's role has sufficient permissions to read CloudWatch logs.\n",
    "\n",
    "Assuming the job name is stored in an environment variable `TRAIN_JOB_NAME`, run these CLI commands:\n",
    "\n",
    "```bash\n",
    "# Find out the log-stream name; should look like TRAIN_JOB_NAME/xxx.\n",
    "aws logs describe-log-streams \\\n",
    "    --log-group-name /aws/sagemaker/TrainingJobs \\\n",
    "    --log-stream-name-prefix $TRAIN_JOB_NAME \\\n",
    "    | jq -r '.logStreams[].logStreamName'\n",
    "\n",
    "\n",
    "# Get the log events\n",
    "aws logs get-log-events \\\n",
    "    --log-group-name /aws/sagemaker/TrainingJobs \\\n",
    "    --log-stream-name <LOG_STREAM_NAME>\n",
    "```\n",
    "\n",
    "For processing job, the log group name is `/aws/sagemaker/ProcessingJobs`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
