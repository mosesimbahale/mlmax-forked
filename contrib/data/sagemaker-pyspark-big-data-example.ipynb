{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "level-possible",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Get the raw data\n",
    "- The data used in this example is taken from the [NYC TLC Trip Record Data](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), which was downloaded using the tools provided by https://github.com/toddwschneider/nyc-taxi-data. These tools can be used to download the entire dataset from scratch, or to update an existing dataset.\n",
    "- Upload the dataset to an S3 bucket, the folder structure should look something like this:\n",
    "\n",
    "```\n",
    "$ aws s3 ls s3://BUCKET/csv/ --human-readable\n",
    "                           PRE unaltered/\n",
    "2020-10-13 13:47:21  364.0 KiB central_park_weather.csv\n",
    "2020-10-13 13:47:21   45.7 KiB fhv_bases.csv\n",
    "2020-10-13 13:47:21   81.8 MiB fhv_tripdata_2015-01.csv\n",
    "2020-10-13 13:47:21   93.3 MiB fhv_tripdata_2015-02.csv\n",
    "...\n",
    "2020-10-13 13:51:20    1.2 GiB fhvhv_tripdata_2019-02.csv\n",
    "2020-10-13 13:51:20    1.4 GiB fhvhv_tripdata_2019-03.csv\n",
    "...\n",
    "2020-10-13 13:53:16    1.1 MiB green_tripdata_2013-08.csv\n",
    "2020-10-13 13:53:17    7.3 MiB green_tripdata_2013-09.csv\n",
    "...\n",
    "2020-10-13 13:54:32    2.4 GiB yellow_tripdata_2009-01.csv\n",
    "2020-10-13 13:54:32    2.2 GiB yellow_tripdata_2009-02.csv\n",
    "...\n",
    "2020-10-13 14:21:57   30.2 MiB yellow_tripdata_2020-05.csv\n",
    "2020-10-13 14:21:58   47.9 MiB yellow_tripdata_2020-06.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-penetration",
   "metadata": {},
   "source": [
    "## Submit a SageMaker Processing job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-western",
   "metadata": {},
   "source": [
    "### Create the processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./src/preprocess.py\n",
    "import argparse\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--s3_input_prefix\", type=str)\n",
    "    parser.add_argument(\"--s3_output_prefix\", type=str)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    s3_input_prefix = args.s3_input_prefix.rstrip(\"/\").lstrip(\"s3://\")\n",
    "    s3_output_prefix = args.s3_output_prefix.rstrip(\"/\").lstrip(\"s3://\")\n",
    "   \n",
    "    # Build the spark session\n",
    "    spark = SparkSession.builder \\\n",
    "                        .appName(\"SparkProcessor\") \\\n",
    "                        .getOrCreate()\n",
    "        \n",
    "    # Read the raw input csv from S3 \n",
    "    sdf_fhv = spark.read.csv(f\"s3://{s3_input_prefix}/fhv_tripdata_*.csv\",\n",
    "                             header=True, inferSchema=True)\n",
    "    sdf_fhvhv = spark.read.csv(f\"s3://{s3_input_prefix}/fhvhv_tripdata_*.csv\",\n",
    "                               header=True, inferSchema=True)\n",
    "    sdf_green = spark.read.csv(f\"s3://{s3_input_prefix}/green_tripdata_*.csv\",\n",
    "                               header=True, inferSchema=True)\n",
    "    sdf_yellow = spark.read.csv(f\"s3://{s3_input_prefix}/yellow_tripdata_*.csv\",\n",
    "                               header=True, inferSchema=True)\n",
    "            \n",
    "    sdf0 = sdf_fhv.select(F.col(\"Pickup_date\").alias(\"dt_pickup\")) \\\n",
    "                  .unionAll(sdf_fhvhv.select(F.col(\"pickup_datetime\").alias(\"dt_pickup\"))) \\\n",
    "                  .unionAll(sdf_green.select(F.col(\"lpep_pickup_datetime\").alias(\"dt_pickup\"))) \\\n",
    "                  .unionAll(sdf_yellow.select(F.col(\"Trip_Pickup_DateTime\").alias(\"dt_pickup\")))\n",
    "    \n",
    "    # Generate ride-counts at 15-minute intervals\n",
    "    sdf1 = \\\n",
    "        sdf0.groupBy(F.window(\"dt_pickup\", \"15 minutes\")) \\\n",
    "            .agg(F.count(\"*\").alias(\"num_rides\")) \\\n",
    "            .withColumn(\"timestamp\", F.col(\"window.start\")) \\\n",
    "            .drop(\"window\")\n",
    "\n",
    "    # Add time-based features\n",
    "    sdf2 = \\\n",
    "        sdf1.select(\"*\",\n",
    "                    (F.minute(\"timestamp\") + F.hour(\"timestamp\") * 60).alias(\"minutes\"),\n",
    "                    (F.dayofweek(\"timestamp\")).alias(\"dow\"),\n",
    "                    (F.month(\"timestamp\")).alias(\"month\"),\n",
    "                    (F.dayofyear(\"timestamp\")).alias(\"doy\"))\n",
    "    \n",
    "    # Write features to parquet\n",
    "    sdf2.write \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .parquet(f\"s3://{s3_output_prefix}/X.parquet\", mode=\"overwrite\")\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-subscriber",
   "metadata": {},
   "source": [
    "### Run the processing job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-thong",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "BUCKET = \"YOUR-BUCKET-NAME\"\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chicken-apparatus",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = PySparkProcessor(\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=8,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=sm_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continent-aerospace",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = [{\n",
    "    \"Classification\": \"spark-defaults\",\n",
    "    \"Properties\": {\"spark.executor.memory\": \"4g\"},\n",
    "}]\n",
    "\n",
    "# This takes ~45mins to complete using 8 x ml.m5.xlarge instances\n",
    "processor.run(\n",
    "    submit_app=\"src/preprocess.py\",\n",
    "    arguments=[\"--s3_input_prefix\", f\"s3://{BUCKET}/csv/\",\n",
    "               \"--s3_output_prefix\", f\"s3://{BUCKET}/parquet/\"],\n",
    "    configuration=configuration,\n",
    "    logs=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
